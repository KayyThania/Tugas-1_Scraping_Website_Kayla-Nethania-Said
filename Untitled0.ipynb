{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRigOBNC2YigIfQvL26+mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KayyThania/Tugas-1_Scraping_Website_Kayla-Nethania-Said/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://www.cnnindonesia.com/indeks/\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def get_articles(page_num=1):\n",
        "    url = BASE_URL + str(page_num)\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    articles_data = []\n",
        "\n",
        "    # ambil list artikel\n",
        "    articles = soup.find_all(\"article\", class_=\"list-content\")\n",
        "    for art in articles:\n",
        "        try:\n",
        "            title_tag = art.find(\"h2\", class_=\"title\")\n",
        "            title = title_tag.get_text(strip=True) if title_tag else None\n",
        "\n",
        "            link_tag = art.find(\"a\")\n",
        "            link = link_tag[\"href\"] if link_tag else None\n",
        "            slug = link.split(\"/\")[-1] if link else None\n",
        "\n",
        "            category_tag = art.find(\"a\", class_=\"kanal\")\n",
        "            category = category_tag.get_text(strip=True) if category_tag else None\n",
        "\n",
        "            image_tag = art.find(\"img\")\n",
        "            image = image_tag[\"src\"] if image_tag else None\n",
        "\n",
        "            summary_tag = art.find(\"div\", class_=\"text\")\n",
        "            summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
        "\n",
        "            # scrape detail artikel\n",
        "            article_res = requests.get(link, headers=headers)\n",
        "            article_soup = BeautifulSoup(article_res.text, \"html.parser\")\n",
        "\n",
        "            # isi konten\n",
        "            paragraphs = article_soup.find_all(\"div\", class_=\"detail-text\")\n",
        "            content = \" \".join([p.get_text(strip=True) for p in paragraphs]) if paragraphs else None\n",
        "\n",
        "            # tanggal publikasi\n",
        "            date_tag = article_soup.find(\"div\", class_=\"date\")\n",
        "            time_posted = date_tag.get_text(strip=True) if date_tag else None\n",
        "\n",
        "            # penulis\n",
        "            author_tag = article_soup.find(\"div\", class_=\"author\")\n",
        "            author = author_tag.get_text(strip=True) if author_tag else None\n",
        "\n",
        "            # update terakhir\n",
        "            update_tag = article_soup.find(\"div\", class_=\"update\")\n",
        "            last_updated = update_tag.get_text(strip=True) if update_tag else None\n",
        "\n",
        "            # tags\n",
        "            tags = [tag.get_text(strip=True) for tag in article_soup.find_all(\"a\", class_=\"tag\")]\n",
        "\n",
        "            # sub kategori (dari breadcrumb)\n",
        "            breadcrumb = article_soup.find(\"div\", class_=\"breadcrumb\")\n",
        "            sub_category = breadcrumb.find_all(\"a\")[-1].get_text(strip=True) if breadcrumb else None\n",
        "\n",
        "            articles_data.append({\n",
        "                \"title\": title,\n",
        "                \"slug\": slug,\n",
        "                \"url\": link,\n",
        "                \"category\": category,\n",
        "                \"sub_category\": sub_category,\n",
        "                \"time_posted\": time_posted,\n",
        "                \"last_updated\": last_updated,\n",
        "                \"author\": author,\n",
        "                \"editor\": None,  # CNN biasanya tidak ada editor\n",
        "                \"summary\": summary,\n",
        "                \"content\": content,\n",
        "                \"tags\": tags,\n",
        "                \"image_url\": image,\n",
        "                \"source\": \"CNN Indonesia\"\n",
        "            })\n",
        "\n",
        "            time.sleep(1)  # jeda antar artikel\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return articles_data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    all_data = []\n",
        "    total_pages = 5   # ubah sesuai kebutuhan, misalnya 50 halaman\n",
        "\n",
        "    for i in range(1, total_pages+1):\n",
        "        print(f\"Scraping halaman {i} ...\")\n",
        "        articles = get_articles(i)\n",
        "        all_data.extend(articles)\n",
        "        time.sleep(2)  # jeda antar halaman\n",
        "\n",
        "    # Export ke JSON\n",
        "    with open(\"cnn_news_multi.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Export ke CSV & Excel\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(\"cnn_news_multi.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    df.to_excel(\"cnn_news_multi.xlsx\", index=False)\n",
        "\n",
        "    print(f\"Total artikel terkumpul: {len(all_data)}\")\n",
        "    print(\"Data berhasil diexport ke JSON, CSV, dan XLSX\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnAgj4ts1gQv",
        "outputId": "200fdb08-3f94-4a86-e4f6-0d303061ee52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping halaman 1 ...\n",
            "Scraping halaman 2 ...\n",
            "Scraping halaman 3 ...\n",
            "Scraping halaman 4 ...\n",
            "Scraping halaman 5 ...\n",
            "Total artikel terkumpul: 0\n",
            "Data berhasil diexport ke JSON, CSV, dan XLSX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://news.detik.com/indeks/\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def get_articles(page_num=1):\n",
        "    url = BASE_URL + str(page_num)\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    articles_data = []\n",
        "\n",
        "    # ambil list artikel\n",
        "    articles = soup.find_all(\"article\")\n",
        "    for art in articles:\n",
        "        try:\n",
        "            title_tag = art.find(\"h3\")\n",
        "            title = title_tag.get_text(strip=True) if title_tag else None\n",
        "\n",
        "            link_tag = art.find(\"a\")\n",
        "            link = link_tag[\"href\"] if link_tag else None\n",
        "            slug = link.split(\"/\")[-1] if link else None\n",
        "\n",
        "            category_tag = art.find(\"span\", class_=\"labdate\")\n",
        "            category = category_tag.get_text(strip=True).split(\" - \")[0] if category_tag else None\n",
        "            time_posted = category_tag.get_text(strip=True).split(\" - \")[-1] if category_tag else None\n",
        "\n",
        "            image_tag = art.find(\"img\")\n",
        "            image = image_tag[\"src\"] if image_tag else None\n",
        "\n",
        "            summary_tag = art.find(\"p\")\n",
        "            summary = summary_tag.get_text(strip=True) if summary_tag else None\n",
        "\n",
        "            # scrape detail artikel\n",
        "            article_res = requests.get(link, headers=headers)\n",
        "            article_soup = BeautifulSoup(article_res.text, \"html.parser\")\n",
        "\n",
        "            # isi konten\n",
        "            paragraphs = article_soup.find_all(\"div\", class_=\"detail__body-text itp_bodycontent\")\n",
        "            if not paragraphs:\n",
        "                paragraphs = article_soup.find_all(\"p\")\n",
        "            content = \" \".join([p.get_text(strip=True) for p in paragraphs]) if paragraphs else None\n",
        "\n",
        "            # tanggal publikasi\n",
        "            date_tag = article_soup.find(\"div\", class_=\"detail__date\")\n",
        "            time_posted_detail = date_tag.get_text(strip=True) if date_tag else time_posted\n",
        "\n",
        "            # penulis\n",
        "            author_tag = article_soup.find(\"div\", class_=\"detail__author\")\n",
        "            author = author_tag.get_text(strip=True) if author_tag else None\n",
        "\n",
        "            # editor\n",
        "            editor_tag = article_soup.find(\"div\", class_=\"editor\")\n",
        "            editor = editor_tag.get_text(strip=True) if editor_tag else None\n",
        "\n",
        "            # update terakhir\n",
        "            update_tag = article_soup.find(\"div\", class_=\"read__update\")\n",
        "            last_updated = update_tag.get_text(strip=True) if update_tag else None\n",
        "\n",
        "            # tags\n",
        "            tags = [tag.get_text(strip=True) for tag in article_soup.find_all(\"a\", class_=\"detail__tag\")]\n",
        "\n",
        "            # sub kategori (dari breadcrumb)\n",
        "            breadcrumb = article_soup.find(\"div\", class_=\"breadcrumb\")\n",
        "            sub_category = breadcrumb.find_all(\"a\")[-1].get_text(strip=True) if breadcrumb else None\n",
        "\n",
        "            articles_data.append({\n",
        "                \"title\": title,\n",
        "                \"slug\": slug,\n",
        "                \"url\": link,\n",
        "                \"category\": category,\n",
        "                \"sub_category\": sub_category,\n",
        "                \"time_posted\": time_posted_detail,\n",
        "                \"last_updated\": last_updated,\n",
        "                \"author\": author,\n",
        "                \"editor\": editor,\n",
        "                \"summary\": summary,\n",
        "                \"content\": content,\n",
        "                \"tags\": tags,\n",
        "                \"image_url\": image,\n",
        "                \"source\": \"Detik.com\"\n",
        "            })\n",
        "\n",
        "            time.sleep(1)  # jeda antar artikel\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return articles_data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    all_data = []\n",
        "    total_pages = 5   # ubah sesuai kebutuhan, misalnya 50 halaman\n",
        "\n",
        "    for i in range(1, total_pages+1):\n",
        "        print(f\"Scraping halaman {i} ...\")\n",
        "        articles = get_articles(i)\n",
        "        all_data.extend(articles)\n",
        "        time.sleep(2)  # jeda antar halaman\n",
        "\n",
        "    # Export ke JSON\n",
        "    with open(\"detik_news_multi.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    # Export ke CSV & Excel\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.to_csv(\"detik_news_multi.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "    df.to_excel(\"detik_news_multi.xlsx\", index=False)\n",
        "\n",
        "    print(f\"Total artikel terkumpul: {len(all_data)}\")\n",
        "    print(\"Data berhasil diexport ke JSON, CSV, dan XLSX\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6CH_Gmx63Mu",
        "outputId": "01bc3579-ab81-4dc6-938f-d2c101eb5c36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping halaman 1 ...\n",
            "Scraping halaman 2 ...\n",
            "Scraping halaman 3 ...\n",
            "Scraping halaman 4 ...\n",
            "Scraping halaman 5 ...\n",
            "Total artikel terkumpul: 0\n",
            "Data berhasil diexport ke JSON, CSV, dan XLSX\n"
          ]
        }
      ]
    }
  ]
}